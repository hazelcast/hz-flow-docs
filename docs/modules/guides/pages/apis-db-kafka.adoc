= First integration
:description: Deploy {lpn} locally, then integrate a database, a REST API, and Kafka

== Overview

In this tutorial, we'll set up a local instance of {short-product-name}, and then see how to use {short-product-name}
to link data from a REST API, a database, and a Kafka topic.

NOTE: This is a step-by-step introduction for beginners. If you're already familiar with the basics of {short-product-name}, our other guides may be more appropriate.

Our use case is to find a list of films and discover which online streaming service has the
films available to watch.

This involves linking the Films list from our database with streaming service information from a REST API.

Finally, we'll listen for updates on a Kafka topic, sending Protobuf messages, and join those with the database and API.

Here's what our demo ecosystem looks like:

image:architecture-overview.png[A database, a REST API and a Kafka topic all walk into a bar...]

=== Prerequisites

You should have https://docs.docker.com/engine/install/[Docker] and https://docs.docker.com/compose/install/[Docker Compose]

== Start a local instance of {short-product-name}

In this tutorial, we're going to deploy {short-product-name}, as well as a few demo projects
that we'll use throughout.

Everything you need is packaged up in a Docker Compose file to make getting started easier.  To launch, run the following in your command line:

[,bash]
----
mkdir {code-product-name}-films-demo
cd {code-product-name}-films-demo
wget https://github.com/{code-product-name}api/demos/raw/main/importing-through-ui/docker-compose.yml
wget https://github.com/{code-product-name}api/demos/raw/main/importing-through-ui/docker/schema-server/schema-server.conf -P {code-product-name}/schema-server
wget https://github.com/{code-product-name}api/demos/raw/main/importing-through-ui/docker/schema-server/projects/taxi.conf -P {code-product-name}/schema-server/projects
wget https://github.com/{code-product-name}api/demos/raw/main/importing-through-ui/services.conf -P {code-product-name}/config
docker compose up -d
----

This will download the following files:

* https://gitlab.com/vyne/demos/-/raw/master/films/docker-compose.yml[docker-compose.yml] - The Docker Compose file that contains all the Docker images for this tutorial
* https://gitlab.com/vyne/demos/-/raw/master/films/docker/schema-server/schema-server.conf[schema-server.conf] - A config file for {short-product-name}'s Schema Server, which will store and publish the schemas we generate in this tutorial
* https://gitlab.com/vyne/demos/-/raw/master/films/docker/schema-server/projects/taxi.conf[taxi.conf] - A skeleton Taxi project configuration, which contains basic information for the schemas we create
* https://gitlab.com/vyne/demos/-/raw/master/films/services.conf[services.conf] - A service discovery configuration file to tell services where to find each other. {short-product-name} supports various service discovery mechanisms

After about a minute, {short-product-name} should be available at http://localhost:9022.

To make sure everything is ready to go, head to the http://localhost:9022/projects[Projects Explorer] to make sure that some schemas are registered.
It should look like the screenshot below.  If you see a message saying there's nothing registered, wait a few moments longer.

image:schema-explorer.png[The Projects screen]

You now have {short-product-name} running locally, along with a handful of demo services
which we'll use in our next steps.

If you run `docker ps`, you should see a collection of Docker containers now running.

|===
| Docker Image | Part of {short-product-name} stack or Demo? | Description

| {code-product-name}hq/{code-product-name}
| {short-product-name}
| The core {short-product-name} server, which provides our UI, and runs all our integration for us

| films-api
| Demo
| A REST API, which exposes information about films.  We also publish a Protobuf message to Kafka from here

| pagila-db
| Demo
| A Postgres DB, which contains the Postgres https://github.com/devrimgunduz/pagila[Pagila] demo database for a fake DVD rental store

| confluentinc/cp-zookeeper
| Demo (Kafka)
| Official zookeeper image, required to run Kafka

| confluentinc/cp-kafka
| Demo (Kafka)
| The official Kafka image
|===

=== Related links

* link:/docs/deploying/production-deployments/[Deploying {short-product-name} without the demo projects]
* https://gitlab.com/vyne/demos/-/tree/master/films[Demo project source code, on GitLab]

== Connect a database table

First, we'll add a connection to our database, and make a table available
as a datasource that {short-product-name} can fetch data from.

This demo ships with an instance of the Postgres demo DB called https://github.com/devrimgunduz/pagila[Pagila].

Pagila contains several tables related to running a fictional DVD Rental store, including details of all sorts of different
films, actors, etc.  We'll use this database as part of our walk through.

To get started, click *Add a data source* on the home page of {short-product-name}, or navigate to http://localhost:9022/data-source-manager/add

=== Define the database connection

First, we need to tell {short-product-name} how to connect to the database.

* Navigate to http://localhost:9022/data-source-manager/add
* Select *Database Table* as the data source to add
* For Connection, select *Add a new connection...*
* A pop-up window appears, allowing you to create a connection to our database
* Fill in the form with the details below:

|===
| Parameter | Value

| Connection name
| films-database

| Connection type
| Postgres

| Host
| postgres

| Port
| 5432

| Database
| pagila

| Username
| root

| Password
| admin
|===

* Click *Test connection* and wait for the connection test to be successful.
* Click *Save*.

The connection to the database has now been created, and the pop-up should close.

=== Related links

* link:/docs/deploying/configuring-connections[{short-product-name} data source configuration]
* link:/docs/connecting-data-sources/connecting-a-database[Managing database connections]

=== Select the table to import

Now that {short-product-name} has a connection to the database, we need to select the tables we
want to make available for {short-product-name} to query from.

{short-product-name} will create schema files for the contents of the table.  Specifically, {short-product-name} will create:

* A model for the table, defining all the fields that are present
* A series of types, which describe the content of each field
* A query service, which lets {short-product-name} run queries against the database

To import the schema:

* Complete the form for the database table to import using the parameters below:

|===
| Parameter | Value

| Connection
| `films-database` (Note - this should already be populated from the previous step)

| Table
| `film`

| Default namespace
| `com.petflix.films`
|===

Namespaces are used to help us group related content together - like packages in Java or namespaces in C# and Typescript.

Here, we're providing a default namespace, which will be applied to the types, models and services {short-product-name} will create
importing this table.

* Click *Configure*

{short-product-name} will connect to the database, and create all the necessary schema configuration for us for the table.

image:schema-importer-db.png[Data sources screen]

=== Related links

* https://docs.taxilang.org/language-reference/taxi-language/#namespaces[Understanding namespaces]
* https://docs.taxilang.org/language-reference/types-and-models/[Understanding types and models]

=== Preview the imported tables

{short-product-name} now shows a preview of the types, models and services that will be created.

image:schema-preview.png[Schema view]

Click around to explore the different models, types and services that will
be created. For now, the defaults that have been assigned are good enough.

* Click *Save*.

{short-product-name} will create the necessary schema files in a local project.

{short-product-name} also creates a series of https://taxilang.org[Taxi] schema files that contain the schemas we've just imported. You can explore these files locally. 

----
cd {code-product-name}/schema-server/projects
----

Taxi ships a great https://marketplace.visualstudio.com/items?itemName=taxi-lang.taxi-language-server[VS Code plugin] which provides click-to-navigate, syntax highlighting, autocompletion and more.

You've now connected a database to {short-product-name}, and exposed one of its tables, so that {short-product-name} can
run queries against it.

=== Related links

* https://docs.taxilang.org/language-reference/taxi-language/[Understanding Taxi]

== Connect a Swagger API

In this step, we want to tell {short-product-name} about our REST API, which exposes information about
which streaming service each of our films is available on.

We'll use the UI of {short-product-name} to import a Swagger definition of our REST API

* Click on the {short-product-name} logo in the top-left corner to navigate back to the home page
* Once again, click *Add a data source*
* Alternatively, navigate to http://localhost:9022/data-source-manager/add
* From the drop-down list, select *Swagger / Open API* as the type of schema to import
* For the Swagger Source, select a URL

Fill in the form with the following values:

|===
| Parameter | Value

| Swagger source
| `+http://films-api/v3/api-docs+`

| Default namespace
| `com.petflix.listings`

| Base url
| Leave this blank
|===

* Click *Create*.

=== Update the service type

A preview of the imported schema is once again displayed.

This time, we do need to modify some default values.

Click on *Services* â†’ `getStreamingProvidersForFilmUsingGET`.

This shows the API operation that's exposed in the Swagger spec we just imported.
This API accepts the ID of a film, and returns information about the streaming services that have the film available to watch.

Now, take a look at the parameters section.

Note that the input parameter - `filmId` is typed as `Int`.  Since we know that this is a FilmId (the same value that's exposed
by the Films database table), we need to update the type accordingly, so that {short-product-name} knows these two pieces of information are linked.

* Click on the `Int` link
* In the search box, type `FilmId`
* Select the FilmId type that's shown
* Finally, click *Save*

Great!  We've now exposed the Swagger API to {short-product-name}.

==== What just happened?

We've connected the Swagger schema of a REST API to {short-product-name}.  {short-product-name} now knows about this service, and will
make calls to it as needed.

=== Related links

* link:/docs/connecting-data-sources/schema-publication-methods[Understanding the different ways to publish schemas to {short-product-name}]
* https://docs.taxilang.org/language-reference/describing-services/[Describing REST APIs in Taxi]
* https://docs.taxilang.org/generating-taxi-from-source/#openapi-x-taxi-type-extension[Embedding Taxi definitions inside Swagger, to keep {short-product-name} automatically up to date]

== Integrating services and loading data

Now that everything is set up, let's fetch and integrate some data.

=== List all the films in the database

Queries in {short-product-name} are written in TaxiQL. TaxiQL is a simple query language that
isn't tied to one specific underlying technology (i.e., it's independent of databases, APIs, etc.).

This means we can write queries for data without worrying where the data is served from.

Our first query is very simple - it just finds all the films.

* Head over to the Query Builder, and select the Query Editor tab (or navigate to http://localhost:9022/query/editor)
* Paste in the below query:

[,taxi]
----
find { Film[] }
----

* Click *Run*.

This query asks {short-product-name} for all `Film` records.
When this query is executed, {short-product-name} looks for services that expose a collection of Films, and invokes them.
In our example, this means {short-product-name} will query the database to select all available films.

There are different options to show the result of {short-product-name} queries. These are displayed as tabs under the query editor.

* Table - Nice for tabular, two-dimensional data
* Tree - Nice for nested data
* Raw - Raw JSON - great for larger result sets
* Profile - What work {short-product-name} did to produce the result. Contains information about the systems called by {short-product-name}, performance stats and lineage information

Once the query has completed, a list of records appears in the grid.

image:results-table-1.png[Results table]

=== Transform the data

{short-product-name} lets you restructure data in a way that's useful to you.
Our original query returned the data as a flat list, since it's coming from a database.

However, for our purposes (let's say we're building a UI) we might want to restructure the data
to a subset of fields, grouped in a way that's useful.

* Paste the below query into the Query Editor.

[,taxi]
----
find { Film[] } as {
    film: {
        name: Title
        id : FilmId
        description: Description
    }
    productionDetails: {
        released: ReleaseYear
    }
}[]
----

* Click *Run*.

This time, the data has been returned structured as a tree.  To see the tree data, click on the *Tree* tab in the results panel.

image:results-tree.png[Tree tab]

Our data has now been restructured into a tree shape.
Using this approach, we can change the shape of the structure, along with field names.

In Taxi language, this is called a _projection_ as we're changing the shape of the output.

=== Combine data from our DB and REST API

Finally, let's add in data about which streaming movie service contains each movie.
This requires linking data between our database and our REST API.

As {short-product-name} is handling all of the integration for us, this is as simple as updating our
query to include the provider data.

{short-product-name} works out how to call the REST API, which data to pass, and what to collect.

* Paste the below query:

[,taxi]
----
find { Film[] } as {
    film: {
        name: Title
        id : FilmId
        description: Description
    }
    productionDetails: {
        released: ReleaseYear
    }
    providers: StreamingProvider
}[]
----

* Click *Run*.

When the query results are returned, as this is nested data, ensure you're in the Tree view to see the results.
Note that we now have data from our database, combined with data from our REST API.

image:results-tree-with-providers.png[]

=== Related links

* link:/docs/querying/writing-queries[Writing queries with {short-product-name}]
* https://docs.taxilang.org/language-reference/querying-with-taxiql/[TaxiQL language reference] 

=== Exploring the query execution

{short-product-name} has several diagnostic tools to help us see what happened.

==== Exploring the query execution plan

In the Profiler, click to see the high level integration plan that {short-product-name} used to execute the query,
showing the services that were called, and how data was resolved at a field level.

image:query-lineage.png[]

==== Explore the individual server requests

In the Profiler, click to see a sequence diagram of calls that have taken place to different services.
Clicking on any of the rows shows the actual request and response.

image:call-explorer.png[]

==== Exploring cell-based lineage

{short-product-name} provides detailed trace lineage for each value shown in its results.

In Tree mode, try clicking on one of the names of the streaming providers.  A lineage display will open,
showing the trace of how the value was derived.

* We can see that a value of Netflix was returned from an Http operation
* The input to that Http operation was a FilmId - in our example, the value 1
* Clicking on the FilmId expands the lineage graph to show where that FilmId came from
* We can see that the FilmId was returned as the result of a database query

image:value-lineage.png[]

This deep lineage is very powerful for understanding how data has flowed, and proving the https://en.wikipedia.org/wiki/Data_lineage#Data_provenance[provenance] of data that {short-product-name} is exposing.

=== Running our query via curl

Although {short-product-name}'s UI is powerful, developers will want to interact with {short-product-name} through its API.
That's a topic on its own, but here is an example of running the same query through {short-product-name}'s API, using curl.

==== Getting a JSON payload

We can use curl to get the results of our query as a JSON document.

* Copy and paste the below snippet into a shell window, and press *Enter*:

[,shell]
----
curl 'http://localhost:9022/api/taxiql' \
  -H 'Accept: text/event-stream;charset-UTF-8' \
  -H 'Content-Type: application/taxiql' \
  --data-raw 'find { Film[] } as {
    film: {
        name: Title
        id : FilmId
        description: Description
    }
    productionDetails: {
        released: ReleaseYear
    }
    providers: StreamingProvider
}[]'
----

NOTE: Streaming versus batch results. 
The curl command streams results from {short-product-name} as soon as they're available. That's because we set the `Accept` header to `text/event-stream`. This is both fast, and more efficient for {short-product-name}, as it's not holding results in memory, allowing {short-product-name} to work on arbitrarily large datasets. If you'd rather have the results as a single batch, change the Accept header to `-H 'Accept: application/json'`

=== Related links

* link:/docs/querying/writing-queries/#rest-api[Running queries through {short-product-name}'s API]

== Adding a Kafka streaming source

Now that we have {short-product-name} linking our Database and REST API, it's time we add a Kafka stream into the mix.

We have a new releases topic that emits a message whenever Netflix decides to turn a beloved movie
into a new TV series.

For this part of our demo, we'll use {short-product-name} to listen for new release announcements, and join
data from our REST API and Postgres DB.

=== Import a Protobuf schema

Our new releases topic emits a Protobuf message which {short-product-name} needs to know about.

To keep things simple in our demo, the Protobuf message is available via one of our APIs.  You can view the Protobuf yourself by clicking on http://localhost:9981/proto.
For {short-product-name} (running inside the Docker Compose network), this is visible as `+http://films-api/proto+`.

Import the spec by clicking *Add a data source* on the front page of {short-product-name}, or by navigating to http://localhost:9022/data-source-manager/add.

* Select *Protobuf* as the type of schema to import
* Set the Protobuf Source as a URL
* Paste the URL: `+http://films-api/proto+`
* Click *Create*

You should see a preview of a newly created model: `NewFilmReleaseAnnouncement`.

We need to indicate that the filmId property is the same as the FilmId used elsewhere in our company.

* On the left-hand table, click *Models â†’ NewFilmReleaseAnnouncement*
* The NewFilmReleaseAnnouncement data model is displayed
* In the "Attributes" table, click on the underlined `Integer` type next to filmId
* Search for the type FilmId, and select `film.types.FilmId` from the search box

Also, notice that the announcement field has been typed as `String`.  Let's create a more specific type:

* In the *attributes* table, click on the underlined `String` type next to announcement
* In the popup, click *Create new*

Fill out the form with the following details:

|===
| Parameter | Value

| Type name
| `AnnouncementMessage`

| Namespace
| `com.petflix.announcements`

| Inherits from
| Start typing `String`, and select `lang.taxi.String` from the drop-down box

| Documentation
| Leave this blank for now
|===

image:create-new-type.png[]

* Click *Create type*.

Your `NewReleaseFilmAnnouncement` type should look like the following screenshot:

image:new-release-file-announcement.png[]

* Click *Save*.

We've now imported a Protobuf schema, and linked its fields to other fields in our schema.

=== Import a Kafka topic

Next we need to tell {short-product-name} about the Kafka topic.

* Click the {short-product-name} logo in the navigation bar to return to the {short-product-name} home page.
* Click *Add a Data Source* or navigate to http://localhost:9022/data-source-manager/add
* From the drop-down, select *Kafka Topic*
* In the *Connection Name*, select *Add a new connection...*

Fill out the form with the following details:

|===
| Parameter | Value

| Connection name
| `my-kafka`

| Connection type
| `kafka` (should already be populated)

| Broker address
| `kafka:29092`

| Group Id
| `vyne` (should already be populated)
|===

* Click *Create*.  A new Kafka connection is created, and the popup closes.

Fill out the rest of the form with the following details:

|===
| Parameter | Value

| Connection name
| `my-kafka` (should have been populated when the pop-up closed)

| Topic
| `releases`

| Topic Offset
| `LATEST`

| Namespace
| `com.petflix.announcements`

| Message Type
| `NewFilmReleaseAnnouncement`

| Service and Operation Name
| Leave these blank
|===

* Click *Create*.

A preview of the schema is shown.

By clicking *Services â†’ MyKafkaService â†’ consumeFromReleases*, you can see
a new operation has been created which returns a `Stream<NewFilmReleaseAnnouncement>`.

Streams are a different type of operation. Rather than request / response like an HTTP operation exposes, these
expose a continuous stream of data.

Take a look around, and then click *Save*.

== Join data from Kafka, API and our DB

It's time to explore writing some queries that join data from across all three sources.

First, let's start with querying our Kafka topic. Head over to the http://localhost:9022/query/editor[Query Editor],
and paste the following query:

[,taxi]
----
stream { NewFilmReleaseAnnouncement }
----

You should see results streaming in, which are being published to our Kafka topic.

image:streaming-query-simple.png[]

Now, let's enrich our Kafka stream with data from our other sources.

Cancel the running query, and paste the following:

[,taxi]
----
stream { NewFilmReleaseAnnouncement } as {
    // The announcement comes from our Kafka Protobuf message
    news: {
        announcement: NewFilmReleaseAnnouncement
    }
    // Grab some film information from the Database
    film: {
        name: Title
        id : FilmId
        description: Description
    }
    productionDetails: {
        released: ReleaseYear
    }
    // And query the REST API to see where we can watch this
    providers: StreamingProvider
}[]
----

In the results panel, you should see the following:

image:streaming-data.png[]

Looking in the Profiler tab, you can see the updated integration plan:

image:query-lineage-with-kafka.png[]

=== What just happened?

* {short-product-name} read our Protobuf message from the Kafka topic
* It enriched it with data from a database query
* It then fleshed it out with information from a REST API call
* And served it up in our UI

== What's next?

In this tutorial, we've set up {short-product-name} and used it to automatically integrate data from a Postgres Database, a REST API, and a Kafka topic with Protobuf.  Nice stuff!

=== Look under the hood

To get a better understanding of what happened under the hood, take a look at some of the files that {short-product-name}
has generated during this tutorial.

|===
| Directory | What's there?

| `./vyne/schema-server/schema-server.conf`
| The config file that's driving the Schema server. It defines where to read and write the schema files {short-product-name} created in the background

| `./vyne/schema-server/projects`
| The schema project that {short-product-name} was writing schemas to

| `./vyne/config/connections.conf`
| A connections file defining the API connections you imported in the UI
|===

////
### Other tutorials
There are lots of things left to explore. A few great next tutorials are:

 * <Link to="/reference/schema-server/schema-server/">Understand the difference between "pushing" and "pulling" schemas in {short-product-name}</Link>
 * <Link to="/reference/schema-server/schema-server/#publishing-external-schemas">Push schemas to {short-product-name} automatically</Link>
////
