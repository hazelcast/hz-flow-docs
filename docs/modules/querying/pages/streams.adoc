= Build ETL pipelines


<Callout type='warning' title="Deprecated">
  <p>Pipelines are gradually being replaced by <a href={'/docs/querying/streaming-data'}>streaming queries</a>.</p>
  
  The pipeline syntax is still supported, but we aim to migrate functionality to streaming queries.
  <br />
  For workloads that are oriented around streaming data (such as reading from Kafka), use a streaming query instead. 
</Callout>


## Overview

Where queries in {short-product-name} are great for on-demand data transformation, pipelines provide continuously streaming data between source and destination, using {short-product-name} to transform data along the way.


Pipelines work by reading content from a source, transforming it using {short-product-name}, then writing to a sink.

Pipelines can be defined either in a [config file](#pipeline-config-spec), or using the UI.

## Starting pipeline engine
The pipeline engine ships as a separate Docker image.

A typical Docker Compose config looks as follows:

```yaml
version: "3.3"
services:
  ## Other services omitted
  pipeline-engine:
    image: {code-product-name}/pipeline:latest
```

## Creating a pipeline
Pipelines are created and stored as a series of HOCON files, within a Taxi project.


```hocon taxi.conf
   name: com.demo/pipelines
   version: 0.1.0
   sourceRoot: src/
   additionalSources: {
        "@{short-product-name}/config" : "{short-product-name}/config/*.conf",
>       "@{short-product-name}/pipelines" : "pipelines/*.conf"
   }
```

Pipelines are defined as an Input (transport + spec) and one or more Outputs (transport + spec), with {short-product-name} handling the transformation and enrichment.

```hocon pipeline.conf
// An example pipeline, which watches a local directory for new files.
// For each file, an integration is performed using a transformation,
// and then written to a db
pipelines: [{
      "name": "Filewatcher to db",
      "id" : "file-watch",
      "input": {
        "type": "fileWatcher",
        "direction": "INPUT",
        "typeName" : "Film",
        "path" : ${config.pipelines.watchedPath}
      },
      "transformation" : """
          find { Films } as { 
            title: FilmTitle
            director: DirectorName
            imdbId: ImdbId
            duration: DurationMillis? = first(TrackObject[]) as DurationMillis 
          }
      """,
      "outputs": [{
        "type": "jdbc",
        "direction": "OUTPUT",
        "connection": "films",
        "tableName": "Films"
      }]
}]

```

Pipelines listen on the input, and as messages arrive they are passed to {short-product-name} to transform to
the output models.

The data is transformed, and {short-product-name}'s query engine is used to enrich and populate any missing data.

Although all pipelines are persisted as JSON files, they can be created either through the UI,
or authored as JSON directly.

### Creating a pipeline through configuration
Pipelines are authored and stored in JSON files, with a file per pipeline.

Files should adhere to the pipeline spec, which has its own dedicated [reference documentation](/docs/pipelines/reference).

